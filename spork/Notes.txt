7/11 - working, removed this_worker

   - should report sizes of sharded data with ls -s
     - new FileSystem command "ls <FILEPATTERN>"
     - in FileSystem, branch of sshp that returns all the stdouts of "ls -s <FILE>"
        - which are lines: "<N> <FILE>", N is #blocks
	- output: total blocks, avg blocks/worker, max blocks/worker, max/average
	

   - robustness (but ec2 is 99.99 reliable - reddit says O(1) failures/wk with O(1000) instances)
     - 1 in cloud.ssh* commands:
       - have num mappers / reducers be different from num workers
       - keep track of what is running where
       - ensure a separate monitoring process is running
         - monitor checks in on workers periodically and on failure
	   - removes worker from active list
	   - re-runs failed process elsewhere
     - 2 in progress_bar
       - test to see if a process is an outlier wrt completion
         - if it is then restart elsewhere
	   - if that works then mark worker as unreliable

   - logging: 
      - _report should save process logs from worker invocations


7/7
 - logic all works BUT gets the wrong answer somehow for wordcount
    - problem was that hash can be different on different machines

 - todo
   - this_worker - can I replace with a call to socket.gethostname() ?

7/3
 - updated the config stuff
 - wrote a Make command to launch a cluster
   - couldn't ssh in to the cli-launched cluster though, may need some additional tweaking

7/2
 Status: partly working but hz_worker.py do_map_and_scatter doesn't launch
 jobs correctly ... mainly ssh is needs to be slacker about keys and not ask
 so much, need to add "-o StrictHostKeyChecking=no" to all the ssh stuff
 

 Cluster setup
 - launch from console https://us-east-2.console.aws.amazon.com/ec2/home?region=us-east-2#Instances:v=3;$case=tags:true%5C,client:false;$regex=tags:false%5C,client:false;sort=desc:instanceState
 - rm workers.txt; make workers.txt
 - py hazsoup.py cloud setattr workers "`cat workers.txt`"

6/30

Commands/ptrs for aws ec2
 - console https://us-east-2.console.aws.amazon.com/ec2/home?region=us-east-2#Instances:v=3;$case=tags:true%5C,client:false;$regex=tags:false%5C,client:false;sort=desc:instanceState
 - get instance ips:
   aws ec2 describe-instances | grep -i publicipadd | cut -d\" -f4
 - stop instances
    aws ec2 stop-instances --instance-ids `aws ec2 describe-instances | grep -i instanceid | cut -d\" -f4`
 - log in 
    ssh -i hazsoup.pem ec2-user@ec2-3-144-108-49.us-east-2.compute.amazonaws.com

 - when creating the cluster you need to specify a pre-build keypair (hazsoup)

Blockers:
  - instances have python3.9, but not pip or pip3
  - looks like I will need to install each of those using 'yum'
  - I was able to ssh into an Amazon linux cluster, but not a

  - used: sudo yum install python3-pip -y

https://google.github.io/python-fire/guide/

in gpig
 - views (=rdds) have 'checkpoints' and 'checkpoint plans'
 - views produce a storagePlan which is executed to produce the relation
 - a plan 
    - is compiled to a script that can be executed by the shell
      - compilers treat multi-input commands separately
    - is steps that are grouped into AbstractMapReduceTask
    - an AbstractMapReduceTask
       - has distribute steps, 0/1 map step, and 0/1 reduce step
         - distribute steps are special 'hadoop fs -getmerge SRC DST'

the main
 - allows you to call it with args '-do METHOD.ARG -view v' which means
   to perform some part of the preprocessing for a view
   - doStoreKeyedRows for map/reduce
   - doStoreRows
   - doDistinctMap
   - rowGenerator methods all read from stdin

micro spork:
 - adds machinery to produce plan and execute it

New idea: have separate map-reduce method using fire

class Context:
    workers: list
    environment:
    ...
      def distribute(source: Iterator, dest: str):
      	  """
	  """
    
         

define MyClass(MapReduce):
   input: File
   reducer_input: File
   output: File
   def map(x:any):
       """Yields stuff."""
   def reduce(key:any, value:Iterator)
       """Returns one thing."""
