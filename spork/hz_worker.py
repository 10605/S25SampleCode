from collections.abc import Iterator
import json
import shlex
from subprocess import Popen, PIPE, check_call
import sys
import os

import reduce_util as ru

class Worker:
    """An abstract worker for map-reduce tasks.
    """

    # abstract routines to be implemented by subclasses

    def map(self, x):
        """Yield one or more items. 
        """
        assert False, unimplemented

    def reduce(self, key, values: Iterator):
        """Yield one or more items to be associated with the key.
        """
        assert False, 'unimplemented'


    # invoke with py -m fire hs_wc.py MyWorker do_map SRC DST

    def do_map(self, src, dst):
        """
        """
        with open(dst, 'w') as fp:
            for line in open(src):
                for x in self.map(line):
                    fp.write(str(x) + '\n')

    def _load_config(self, config_file):
        with open(config_file) as fp:
            return json.load(fp)

    def _shard_bufname(self, src, worker_idx):
        stem = os.path.basename(src)
        return f'sortout-{stem}-from-w{worker_idx:02d}.tsv'

    def do_map_and_shuffle(self, src:str, config_file:str, this_worker:str):
        """Run mapper on src and distribute shards to co-workers. 

        Distributed shards are sorted by key, and named from-{this_worker}-{dst}.
        """
        config = self._load_config(config_file)
        coworkers = config['workers']
        keypair_file = config['keypair_file']
        cloud_username = config['cloud_username']

        # set up destination processes
        shard_buf = self._shard_bufname(src, coworkers.index(this_worker))
        def remote_sort_cmd(worker):
            """A process that accepts lines to sort from stdin.
            """
            local_sort_cmd = ru.sort_command(src='', dst=shard_buf)
            return f'ssh -i {keypair_file} {cloud_username}@{worker} {local_sort_cmd}'
        coworker_processes = [
            Popen(remote_sort_cmd(worker), text=True, stdin=PIPE, shell=True)
            for worker in coworkers
        ]

        # run the map and distribute the data to the processes
        for line in open(src):
            for key, val in self.map(line):
                worker_idx = ru.kv_keyhash(key) % len(coworkers)                
                kv_line = ru.kv_to_line(key, val)
                coworker_processes[worker_idx].stdin.write(kv_line)
        # close the worker processes
        for proc in coworker_processes:
            proc.stdin.close()
            proc.wait()
        
    def do_gather_reduce(self, src, dst, workers):
        """Merge shards generated by do_map_and_shuffle and then reduce.
        """
        config = self._load_config(config_file)
        coworkers = config['workers']
        keypair_file = config['keypair_file']
        cloud_username = config['cloud_username']
        
        # merge-sort the generated shards using sort with -m option
        incoming_shards = [
            self._shard_bufname(src, i) for i in range(len(coworkers))]
        merge_dst =  f'mergeout-{stem}.tsv'
        merge_sort_cmd = ru.sort_command(
            src=' '.join(incoming_shards),
            dst=merge_dst).replace("sort ", "sort -m")
        check_call(merge_sort_cmd, shell=True)

        # create a generator for the sorted pairs 
        def pair_generator():
            for line in open(merge_result_file):
                yield ru.kv_from_line(line)

        # convert pair_generator and invoke and save reduce outputs
        with open(dst, 'w') as fp:
            for key, values in ru.ReduceReady(pair_generator()):
                for reduced_value in self.reduce(key, values):
                    pair = (key, reduced_value)
                    fp.write(str(pair) + '\n')
