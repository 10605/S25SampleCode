from collections.abc import Iterator
import json
import shlex
from subprocess import Popen, PIPE, check_call
import sys
import os

import reduce_util as ru

WORKER_FILENAME = 'workers.txt'
CLOUD_USERNAME = 'ec2-user'
KEYPAIR_FILE = 'hazsoup.pem'

class CloudBase:
    """Base class for working with an ec2 cloud.
    """
    def __init__(self):
        """Loads worker names from workers.txt
        """
        self.worker_file = WORKER_FILENAME
        self.cloud_username = CLOUD_USERNAME
        self.keypair_file = 'hazsoup.pem'
        try:
            with open(self.worker_file) as fp:
                self.workers = [
                    worker.rstrip() for worker in fp.readlines()]
        except FileNotFoundError:
            print(f'warning: no config at {worker_file}')
            self.workers = None

    def ssh_args(self, worker) -> str:
        """Arguments for an ssh/scp command invoking the worker.
        """
        return (f'-i {self.keypair_file} -o StrictHostKeyChecking=no'
                + f' {self.cloud_username}@{worker}')

    def worker_ssh_toks(self, worker, shell_cmd) -> list[str]:
        """Returns the tokens in an ssh command invoking a worker.
        """
        return (
            ['ssh'] 
            + shlex.split(self.ssh_args(worker))
            + shlex.split(shell_cmd)
        )

    def worker_scp_toks(self, worker, filenames) -> list[str]:
        """Returns the tokens to scp the files to the worker.
        """
        # add colon to machine name to specify default dir for scp dst
        ssh_args = shlex.split(self.ssh_args(worker))
        scp_args = ssh_args[:-1] + [f'{ssh_args[-1]}:']
        return (
            ['scp']
            + scp_args[:-1]
            + shlex.split(filenames) 
            + [scp_args[-1]]
        )

class Worker(CloudBase):
    """An abstract worker for map-reduce tasks.
    """

    # abstract routines to be implemented by subclasses

    def map(self, x):
        """Yield one or more items. 
        """
        assert False, unimplemented

    def reduce(self, key, values: Iterator):
        """Yield one or more items to be associated with the key.
        """
        assert False, 'unimplemented'


    # invoke with py -m fire hs_wc.py MyWorker do_map SRC DST

    def do_map(self, src, dst):
        """
        """
        with open(dst, 'w') as fp:
            for line in open(src):
                for x in self.map(line):
                    fp.write(str(x) + '\n')

    def _shard_bufname(self, src, worker_idx):
        stem = os.path.basename(src)
        return f'sortout-{stem}-from-w{worker_idx:02d}.tsv'

    def do_map_and_shuffle(self, src, this_worker):
        """Run mapper on src and distribute shards to co-workers. 

        Distributed shards are sorted by key, and named from-{this_worker}-{dst}.
        """
        coworkers = self.workers
        # set up destination processes - using shell=True to allow
        # LC=all options to sort to be passed in
        def sort_pipe_command_str(worker):
            sort_command = ru.sort_command(
                src='', 
                dst=self._shard_bufname(src, coworkers.index(this_worker)))
            result = f'ssh {self.ssh_args(worker)} {sort_command}'
            print('** sort', result)
            return result
        coworker_processes = [
            Popen(
                sort_pipe_command_str(worker),
                text=True, stderr=PIPE, stdin=PIPE, shell=True)
            for worker in coworkers
        ]
        # run the map and distribute the data to the processes
        for line in open(src):
            for key, val in self.map(line):
                worker_idx = ru.kv_keyhash(key) % len(coworkers)                
                kv_line = ru.kv_to_line(key, val)
                coworker_processes[worker_idx].stdin.write(kv_line)
        # close the worker processes
        for proc, worker in zip(coworker_processes, coworkers):
            proc.stdin.close()
            proc.wait()
            if proc.returncode:
                print(f'returncode {worker}: {proc.returncode}')
            error_log = proc.stderr.read()
            if error_log:
                print(f'stderr {worker}'.center(60, '='))
                print(error_log, end='')
        
    def do_gather_reduce(self, src, dst, worker_file):
        """Merge shards generated by do_map_and_shuffle and then reduce.
        """
        coworkers = self.workers
        
        # merge-sort the generated shards using sort with -m option
        incoming_shards = [
            self._shard_bufname(src, i) for i in range(len(coworkers))]
        stem = os.path.basename(src)
        merge_dst =  f'mergeout-{stem}.tsv'
        merge_sort_cmd = ru.sort_command(
            src=' '.join(incoming_shards),
            dst=merge_dst).replace("sort ", "sort -m ")
        check_call(merge_sort_cmd, shell=True)

        # create a generator for the sorted pairs 
        def pair_generator():
            for line in open(merge_dst):
                yield ru.kv_from_line(line)

        # convert pair_generator and invoke and save reduce outputs
        with open(dst, 'w') as fp:
            for key, values in ru.ReduceReady(pair_generator()):
                for reduced_value in self.reduce(key, values):
                    pair = (key, reduced_value)
                    fp.write(str(pair) + '\n')
