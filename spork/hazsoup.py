"""Very simple map-reduce system.
"""

import ast
import collections
import fire
import json
import os
from pprint import pprint
from subprocess import Popen, PIPE
from subprocess import run as run_subproc
import shlex
import sys
import time
from tqdm import tqdm

WORKER_PYTHON = 'python3'
DEFAULT_CONFIG_FILE = 'cloud_config.json'

class Worker:
    """An abstract worker for map-reduce tasks.
    """

    # abstract routines to be implemented by instances via worker.map
    # and then executed by commands like

    # py -m fire hs_wc.py MyWorker do_map DIR SRC DST

    def do_map(self, src, dst):
        """
        """
        with open(dst, 'w') as fp:
            for line in open(src):
                for x in self.map(line):
                    fp.write(x + '\n')

    def map(self, x):
        """Yield one or more items. 

        Map will be called repeatedly and outputs will be sent to the
        reducer in a map-reduce, or else serialized and written out to
        the destination file for a map-only job.
        """
        assert False, unimplemented

    # TODO
    def scatter_keyed_shards(self, cwd, src_worker_name, src, dst, workers):
        """Shard src by key, and send shards to workers for sorting by key.

        Names of shards are {dst}-from-{src_worker_name} so they
        can be collected.
        """
        ...
        
    # TODO
    def do_gather_reduce(self, src, dst, workers, wd):
        """Merge shards generated by scatter_keyed_shards and reduce.

        Specifically, merge labeled {src}-from-{worker}, and stream
        the output of that sort process to the self.reduce(key,
        values).
        """
        ...

    def reduce(self, accum, value2, initial_accum=lambda x:x):
        """
        """
        assert False, 'unimplemented'

class CloudBase:
    """Base class for working with an ec2 cloud.
    """
    def __init__(self, config_file=DEFAULT_CONFIG_FILE):
        """Load state previously specified with 'config' from disk.
        """
        try:
            with open(config_file) as fp:
                config = json.load(fp)
            for key, value in config.items():
                setattr(self, key, value)
        except FileNotFoundError:
            print(f'warning: no config at {config_file}')
            self.cloud_username = 'ec2-user'
            self.keypair_file = 'hazsoup.pem'
            self.workers = None

    def _defined_attr(self):
        """Externally visible attributes of this object.
        """
        return [a for a in self.__dict__ if not a.startswith('_')]

    def _save(self, config_file=DEFAULT_CONFIG_FILE):
        """Save the defined attribute values to the config file.
        """
        with open(config_file, 'w') as fp:
            config = {a:getattr(self, a) for a in self._defined_attr()}
            json.dump(config, fp)
        print(f'saved to {config_file}: {self._defined_attr()}')

    def setattr(self, attr, value, split=True, config_file=DEFAULT_CONFIG_FILE):
        """Set an attribute, like workers, keypair_file, or cloud_username.
        """
        if split:
            value = value.split()
        setattr(self, attr, value)
        self._save()

    def _report(self, proc, worker, stderr_pipe=False, stdout_pipe=False):
        if proc.returncode:
            print(f'returncode {worker}: {proc.returncode}')
        def report_stdx(what, worker, wrapper_or_string, flag):
            outp = wrapper_or_string.read() if flag else wrapper_or_string
            if outp:
                print(f'{what} {worker}'.center(60, '='))
                print(outp, end='')
        report_stdx('stdout', worker, proc.stdout, stdout_pipe)
        report_stdx('stderr', worker, proc.stderr, stderr_pipe)

    def run_all(self, command):
        """Run a shell command on all workers.
        """
        for worker in self.workers:
            sh_tokens = (['ssh', '-i', self.keypair_file]
                         + [f'{self.cloud_username}@{worker}']
                         + shlex.split(command))
            proc = run_subproc(sh_tokens, capture_output=True, text=True)
            self._report(proc, worker)
                
    def upload_all(self, filenames):
        """Copy a file to all workers.
        """
        for worker in tqdm(self.workers):
            sh_tokens = (
                ['scp', '-i', self.keypair_file]
                + shlex.split(filenames)
                + [f'{self.cloud_username}@{worker}:'])
            proc = run_subproc(sh_tokens, capture_output=True, text=True)
            self._report(proc, worker)

    def init_ec2(self):
        """Commands needed to initialize an ec2 cluster.
        """
        self.run_all("sudo yum install python3-pip -y")
        self.run_all("pip3 install fire")
        self.upload_all("hz_worker.py")


class FileSystem(CloudBase):

    def put(self, src, dst):
        """Shard a local file and upload shards to the workers.
        """
        line_ctr = collections.Counter()
        # create a process for each worker that can accept text lines
        # via its stdin
        def sh_tokens(worker):
            return (['ssh', '-i', self.keypair_file]
                    + [f'{self.cloud_username}@{worker}']
                    + shlex.split(f'cat > {dst}'))
        worker_processes = [
            Popen(sh_tokens(worker), text=True, stdin=PIPE)
            for worker in self.workers
        ]
        # upload the local file
        for line in tqdm(open(src)):
            # send line to the worker with this index
            worker_idx = hash(line) % len(self.workers)
            worker_processes[worker_idx].stdin.write(line)
            # record some statistics
            line_ctr[src] += 1
            line_ctr[f'{self.workers[worker_idx]}:{dst}'] += 1
        # close the worker processes and wait for them to finish
        for proc in tqdm(worker_processes):
            proc.stdin.close()
            proc.wait()
        # echo statistics
        pprint(line_ctr)
        
    def get_merge(self, src, dst):
        """Get remote shards and collect them into a local file.
        """
        line_ctr = collections.Counter()
        def sh_tokens(worker):
            return (['ssh', '-i', self.keypair_file]
                    + [f'{self.cloud_username}@{worker}']
                    + shlex.split(f'cat < {src}'))
        with open(dst, 'w') as fp:
            for worker in tqdm(self.workers):
                # download the data from that worker to local file
                proc = Popen(
                    sh_tokens(worker), text=True, stdout=PIPE)
                for line in proc.stdout:
                    fp.write(line)
                    # record some statistics
                    line_ctr[f'{worker}:{src}'] += 1
                    line_ctr[dst] += 1
                # wait for process to end
                proc.wait()
        # echo statistics
        pprint(line_ctr)

    def head(self, src):
        worker = self.workers[0]
        proc = run_subproc(
            f'ssh {worker} "head {self._worker_file(src)}"',
            capture_output=True, text=True, shell=True)
        print(proc.stdout, end='')

    def tail(self, src):
        worker = self.workers[-1]
        proc = run_subproc(
            f'ssh {worker} "tail {self._worker_file(src)}"',
            capture_output=True, text=True, shell=True)
        print(proc.stdout, end='')

        

# TODO
class Driver(CloudBase):
    """Invokes the workers appropriately to complete a task.
    """

    def map_only(self, src, dst, main_script, main_class):
        """Run a map-only job.
        """
        def map_command(worker):
            return ['ssh', '-i', self.keypair_file,
                    f'{self.cloud_username}@{worker}',
                    WORKER_PYTHON, '-m', 'fire', main_script, main_class, 
                   'do_map', 
                    '--src', src,
                    '--dst', dst] 
        processes = [
            Popen(map_command(worker), text=True, stderr=PIPE, stdout=PIPE)
            for worker in self.workers]
        for worker in self.workers:
            print('launched:', ' '.join(map_command(worker)))
        for proc, worker in zip(processes, self.workers):
            proc.wait()
            self._report(proc, worker, stderr_pipe=True, stdout_pipe=True)

    def map_reduce(self, src, dst, scripts):
        ...
    
if __name__ == "__main__":
    if len(sys.argv) > 1:
        fire.Fire(dict(
            fs = FileSystem,
            run = Driver,
            cloud = CloudBase))
